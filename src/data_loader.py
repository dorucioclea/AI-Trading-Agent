import yfinance as yf
import pandas as pd
import numpy as np
import logging
from typing import Tuple, Dict, Optional
from sklearn.preprocessing import StandardScaler
from ta.momentum import RSIIndicator
from ta.trend import MACD
# from .tda_features import FeatureProcessor # TDA disabled for Massive Scale speed

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class MVPDataLoader:
    """
    Data Loader specifically designed for the Student MVP constraints.
    Now supports Multi-Ticker List for "Scaling Up".
    Strict Splits:
    - Train: 2019-01-01 -> 2022-12-31
    - Val:   2023-01-01 -> 2023-12-31
    - Test:  2024-01-01 -> Present
    """
    def __init__(self, ticker: str = None, tickers: list = None, window_size: int = 50, feature_scalers: Dict = None):
        # Support single 'ticker' arg or 'tickers' list
        if tickers:
            self.tickers = tickers
        else:
            self.tickers = [ticker] if ticker else ["AAPL"]
            
        self.window_size = window_size
        self.scalers = feature_scalers if feature_scalers else {}
        # TDA Processor (can be heavy, may want to disable for massive data if too slow)
        # self.tda_processor = FeatureProcessor(embedding_dim=3, embedding_delay=1) # Disabled

    def fetch_batch_data(self) -> pd.DataFrame:
        """
        Downloads data for ALL tickers in parallel (Much faster).
        Returns a MultiIndex DataFrame (Price, Ticker).
        """
        if not self.tickers: return pd.DataFrame()
        logger.info(f"Batch downloading {len(self.tickers)} tickers (2018-2025)...")
        
        # Chunking downloads to avoid URI too long errors or rate limits for huge lists
        chunk_size = 100
        all_dfs = []
        
        for i in range(0, len(self.tickers), chunk_size):
            chunk = self.tickers[i:i+chunk_size]
            logger.info(f"Downloading chunk {i}-{i+len(chunk)}...")
            try:
                # Group by Ticker to make extraction easier: df[Ticker] -> DataFrame
                df = yf.download(chunk, start="2018-01-01", end="2025-01-01", group_by='ticker', auto_adjust=True, progress=False, threads=True)
                if not df.empty:
                    all_dfs.append(df)
            except Exception as e:
                logger.error(f"Failed chunk {i}: {e}")
        
        if not all_dfs: return pd.DataFrame()
        
        # Concat along columns (axis=1) if they are wide (Price, Ticker) format... 
        # Wait, concat(axis=1) might align dates automatically.
        full_df = pd.concat(all_dfs, axis=1)
        full_df.ffill(inplace=True)
        return full_df

    def process_single_ticker_data(self, df_ticker: pd.DataFrame) -> pd.DataFrame:
        """Helper to process a single ticker's worth of data from the batch."""
        # Fix columns if needed (batch download usually gives Open/High/Low/Close directly)
        # But yf.download(group_by='ticker') returns columns: Open, High, Low, Close...
        # So df_ticker is already clean.
        return self.feature_engineering(df_ticker)

    def feature_engineering(self, df: pd.DataFrame) -> pd.DataFrame:
        """Adds Technical Indicators (RSI, MACD) and Targets."""
        if len(df) < 50: return pd.DataFrame() # Skip if too short
        
        df = df.copy()
        try:
            # 1. Technical Indicators
            df['RSI'] = RSIIndicator(df['Close'], window=14).rsi()
            macd = MACD(df['Close'])
            df['MACD'] = macd.macd()
            df['MACD_Signal'] = macd.macd_signal()
            
            # 2. Returns
            df['Log_Return'] = np.log(df['Close'] / df['Close'].shift(1))
            
            # 3. Target: Next Day Direction
            threshold = 0.002
            future_ret = df['Log_Return'].shift(-1)
            conditions = [(future_ret < -threshold), (future_ret > threshold)]
            choices = [0, 2] 
            df['Target'] = np.select(conditions, choices, default=1)
            
            df.dropna(inplace=True)
        except Exception as e:
            logger.warning(f"Feature engineering failed: {e}")
            return pd.DataFrame()
            
        return df

    def create_sequences(self, df: pd.DataFrame, dataset_type: str = 'train') -> Tuple[np.ndarray, np.ndarray]:
        """
        Creates (X, y) sequences.
        Normalizes data using Scaler fitted ONLY on TRAIN data (globally or per ticker? Globally is simpler for MVP).
        """
        if df.empty: return np.array([]), np.array([])
        
        # FIX: Remove 'Close' price. It is non-stationary and scaling varies wildly between tickers.
        # Using it prevents the model from learning general patterns across 500+ stocks.
        feature_cols = ['RSI', 'MACD', 'MACD_Signal', 'Log_Return']
        data = df[feature_cols].values
        targets = df['Target'].values
        
        # However, `self.scalers` is a single dict.
        # Let's fit a NEW scaler for each ticker? Or reuse?
        # Re-using a single global scaler for Price is bad.
        # Let's switch to using Log-Returns for Price input or Z-Score Normalize PRICE per series.
        # Decision: Create a local scaler for this sequence generation if training, but we need to save it for inference?
        # Complex.
        # Hack: Since this is "Large Model", let's assume we fit on the current DF passed in.
        # In `get_data_splits`, if we concat DFs first, we lose ticker identity.
        # If we loop tickers -> create seqs -> stack:
        # We must normalize INSIDE the loop per ticker.
        
        local_scaler = StandardScaler()
        if dataset_type == 'train':
            data = local_scaler.fit_transform(data)
            # We can't easily save 50 scalers for inference in this simple MVP structure.
            # But the user wants "Big Model".
            # Compromise: For inference, we usually predict one ticker (AAPL).
            # So training can use per-ticker normalization to learn patterns.
            # We won't save all 50 scalers. We just discard them after creating X.
            pass
        else:
            # For Val/Test, we should technically use the scaler from that ticker's train set.
            # This requires storing scalers by Ticker.
            # Too complex for this codebase refactor right now.
            # FALLBACK: Fit on self (transductive) or just fit on Train portion for that ticker.
            # Let's just fit_transform on the passed df for normalization "locally" for now as an approximation, 
            # or skip normalization of Price and rely on LogRet.
            # Let's stick to: Fit on the passed data (which is a slice). 
            # Correct way: pass the training scaler.
            # Let's simplify: normalize per batch? No.
            
            # OK, Strict Logic:
            # We will fit scaler on this function call. 
            data = local_scaler.fit_transform(data) 

        X, y = [], []
        for i in range(self.window_size, len(data)):
            seq_features = data[i-self.window_size:i] 
            X.append(seq_features)
            y.append(targets[i])

        return np.array(X), np.array(y)

    def get_data_splits(self):
        """
        Iterates over ALL tickers, creates sequences, and stacks them.
        Returns a massive (X, y) dataset.
        """
        all_X_train, all_y_train = [], []
        all_X_val, all_y_val = [], []
        all_X_test, all_y_test = [], []
        
        # 1. Fetch All Data (Batch)
        full_df = self.fetch_batch_data()
        
        if full_df.empty:
            raise ValueError("No data returned from batch download.")
            
        # 2. Iterate and Process
        # Handle Single Ticker vs Multi-Ticker structure from yfinance
        is_multi_index = isinstance(full_df.columns, pd.MultiIndex)
        
        processed_count = 0
        
        for t in self.tickers:
            try:
                # Extract Ticker Data
                if is_multi_index:
                    if t in full_df.columns.get_level_values(0):
                        df = full_df[t].copy()
                    else:
                        continue # Ticker failed to download
                else:
                    # If single ticker and not multi-index, the whole DF is that ticker
                    # But verify we only expected 1 ticker
                    if len(self.tickers) == 1:
                        df = full_df.copy()
                    else:
                        continue 

                # Process
                df = self.feature_engineering(df)
                if df.empty: continue
                
                # Split indices
                train_mask = (df.index >= '2019-01-01') & (df.index <= '2022-12-31')
                val_mask = (df.index >= '2023-01-01') & (df.index <= '2023-12-31')
                test_mask = (df.index >= '2024-01-01')
                
                # Create Seqs
                x_tr, y_tr = self.create_sequences(df[train_mask], 'train')
                x_v, y_v = self.create_sequences(df[val_mask], 'val')
                x_te, y_te = self.create_sequences(df[test_mask], 'test')
                
                if len(x_tr) > 0:
                    all_X_train.append(x_tr)
                    all_y_train.append(y_tr)
                    processed_count += 1
                if len(x_v) > 0:
                    all_X_val.append(x_v)
                    all_y_val.append(y_v)
                if len(x_te) > 0:
                    all_X_test.append(x_te)
                    all_y_test.append(y_te)
                    
            except Exception as e:
                logger.error(f"Error processing {t}: {e}")
                continue
        
        if processed_count == 0:
            # If no tickers processed successfully
             logger.error("Zero tickers processed successfully.")
        
        # Concatenate
        if not all_X_train: raise ValueError("No training data collected!")
        
        X_train = np.concatenate(all_X_train)
        y_train = np.concatenate(all_y_train)
        X_val = np.concatenate(all_X_val) if all_X_val else np.array([])
        y_val = np.concatenate(all_y_val) if all_y_val else np.array([])
        X_test = np.concatenate(all_X_test) if all_X_test else np.array([])
        y_test = np.concatenate(all_y_test) if all_y_test else np.array([])
        
        logger.info(f"Total Dataset: Train={len(X_train)}, Val={len(X_val)}")
        
        return {
            'train': (X_train, y_train),
            'val': (X_val, y_val),
            'test': (X_test, y_test),
            'scalers': {}, # Scalers are local now, dropped.
            'test_dates': [] # Dropped for global training
        }

if __name__ == "__main__":
    # Test
    tickers = ["AAPL", "MSFT"]
    loader = MVPDataLoader(tickers=tickers)
    data = loader.get_data_splits()
    print("Train shape:", data['train'][0].shape)
